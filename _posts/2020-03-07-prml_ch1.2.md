--- 
title: "[PRML] Ch 1.2 Probability Theory"
excerpt: Introduction to probability theory

categories:
    - PRML
tags:
    - Machine Learning
    - Probability
    - Bayes Theorem

use_math: true
toc: true
toc_sticky: true
last_modified_at : 2020-03-09
---
*This post is summary of the book Pattern Recognition and Machine Learning by Christopher M. Bishop*

## 1.2 Probability Theory

- **Uncertainty** is a key concept in the field of pattern recognition
- uncertainty arises through
    - noise on measurements
    - finite size of data sets
- *Probability Theory* provides:
    - a consistent framework for quantification and manipulation of uncertainty
    - forms one of the central foundations for pattern recognition
- when probability theory is combined with *Decision Theory*
    - allows us to make optimal predictions given all the information available on us
    - even though that information may be incomplete or ambiguous
    - will be discussed in Section 1.5

### Example: apple and oranges

- two boxes of fruit from which an item of fruit is randomly selected and replaced in the box from which it came
- suppose this process is repeated many times
    - equally likely to select any of the pieces of fruit in the box
- red box:
    - 2 apples and 6 oranges
    - 40% of the time pick red box
- blue box:
    - 3 apples and 1 orange
    - 60% of the time pick blue box

![figure1.9](../../assets/images/prml/figure1_9)

- random variables:
    - identity of the box:
        - denoted by B
        - possible values:
            - r: red box
            - b: blue box
    - identity of the fruit 
        - denoted by F
        - possible values:
            - a: apple
            - o: orange
- probabilities of incidents:
    - probability of selecting the red box
        - $P(B = r) = \frac{4}{10}$
    - probability of selectinb the blue box
        - $P(B = b) = \frac{6}{10}$
- probabilities must lie in the interval [0,1] by definition
- the sum of probabilities of mutually exclusive events that include all possible outcomes must be one
    - in this example, the chosen box must be either red or blue
    - $P(B) = P(B = r) + P(B = b) = 1$

### The Rules of Probability

- two elementary rules of probability are **sum rule** and **product rule**
- in order to derive these rules of probability, we will consider more general example involving two random variables $X$ and $Y$
    - X can take any values $x_i, i = 1, ..., M$ 
    - Y can taky any values $y_j, j = 1, ..., L$
    - a total of $N$ trials in which we sample both of the variables $X$ and $Y$
    - $n_{ij}$: number of trials which $X = x_i$ and $Y = y_j$
    - $c_i$: number of trials which $X$ takes the value $x_i$ regardless of the value of $Y$
    - $r_j$: number of trials which $Y$ takes the value $y_i$ regardless of the value of $X$

![figure1.10](../../assets/images/prml/figure1_10)

- $p(X = x_i, Y = y_i)$: probability that $X = x_i$ and $Y = y_i$
    - this is called the *joint probability* of $X = x_i$ and $Y = y_i$
    - the number of points falling in the cell $i,j$ as a fraction of the total number of points

$$p(X=x_i, Y=y_i) = \frac{n_{ij}}{N} \tag{1.5}$$

- here, we are implicitly considering the limit $N \rightarrow \infty$
- $p(X=x_i)$: probability that $X$ takes the value $x_i$ irrespective of the value of Y
    - this is called the *marginal probability*
    - the fraction of the total number of points that fall in column $i$

$$p(X=x_i)= \frac{c_i}{N} \tag{1.6} $$

- number of instances in column $i$ in Figure 1.10 is just the sum of the number of instances in each cell of that column
    - $c_i = \sum_j n_{ij}$
    - therefore following equation can be derived from (1.5) and (1.6)

$$ p(X=x_i)= \sum_{j=1}^L p(X=x_i, Y=y_j) \tag{1.7} $$

- this is the *sum rule* of probability

- $p(Y=y_j \vert X=x_i)$: probability of $P(Y=y_j)$ when the value of $X$ is given as $X=x_i$
    - this is called the *conditional probability* of $Y = y_j$ given $X=x_i$
    - the fraction of the point in columne $i$ that fall in cell $i,j$

$$p(Y=y_j \vert X= x_i) = \frac{n_{ij}}{c_i} \tag{1.8}$$

- from (1.5), (1.6), and (1.8), we can derive the following relationship

$$p(X=x_i, Y=y_j) = \frac{n_{ij}}{N} = \frac{n_{ij}}{c_i} \cdot \frac{c_i}{N} = p(Y = y_j \vert X=x_i)p(X=x_i) \tag{1.9}$$
    
- this is the *product rule* of probability

- the rules of probability
    - **sum rule**
    $$p(X) = \sum_{Y} p(X,Y) \tag{1.10}$$
    - **product rule**
    $$p(X,Y) = p(Y|X)p(X) \tag{1.11}$$

### Bayes' Theorem

- from the symmetry property of product rule, we can obtain *Bayes' theorem*
    - symmetry property of product rule

    $$p(X,Y) = p(Y,X) = p(X|Y)p(Y)$$

    - Bayes' Themorm

    $$p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)} \tag{1.12}$$
- Bayes' theorem plays a central role in patter recognition and machine learning
- Bayes' theorem can be expressed in terms of quantities appearing in the numerator using the sum rule
    - denominator can be seen as the normalization constant 
        - because the sum of conditional probability must add up to one 

$$p(X) = \sum_Y p(X|Y)p(Y) \tag{1.13}$$

$$p(Y|X) = \frac{p(X|Y)p(Y)}{\sum_Y p(X|Y)p(Y)}$$

- figure below visualizes joint, marginal, and conditional distributions
    - visualization is important part of statisical pattern recognition and will be explored later

![figure1.11](../../assets/images/prml/figure1_11)

### Summary with numbers

- going back the the apples and oranges example
    - short recap:
        $$p(B=r) = \frac{4}{10} \tag{1.14}$$
        $$p(B=b) = \frac{6}{10} \tag{1.15}$$
- conditional probabilities:
    - the probability of picking apples in the blue box:
    $$p(F=a|B=r)= \frac{1}{4} \tag{1.16}$$
    - the probability of picking oranges in the blue box:
    $$p(F=o|B=r)= \frac{3}{4} \tag{1.17}$$
    - the probability of picking apples in the red box:
    $$p(F=a|B=b) = \frac{3}{4} \tag{1.18}$$
    - the probability of picking oranges in the blue box:
    $$p(F=o|B=b) = \frac{1}{$} \tag{1.19}$$
- note that all possibile events of the given condition must sum up to one
$$p(F=a|B=r) + p(F=o|B=r) = 1 \tag{1.20}$$
$$p(F=a|B=b) + p(F=o|B=b) = 1 \tag{1.21}$$

- sum and product rules of probability to evaluate the overall probability of choosing an apple

$$
\begin{align*}
p(F=a) &= p(F=a|B=r)p(B=r) + p(F=a|B=b)p(B=b) \\
&= \frac{1}{4} \times \frac{4}{10} + \frac{3}{4} \times \frac{6}{10} = \frac{11}{20} \tag{1.22}
\end{align*}
$$

- using the sum rule, $p(F=o) = 1-\frac{11}{20} = \frac{9}{20}$

- suppose we are told which fruit has been selected, and it's orange, and want to find out which box it came from
    - we don't know $p(B=r \vert F=o)$ from the information given
    - but we know $p(F=o \vert B=r)$, $p(B=r)$, and $p(F=o)$
    - therefore, we can use Bayes' theorem to find out the answer for $p(B=r \vert F=o)$

$$
\begin{align*}
p(B=r|F=o) &= \frac{p(F=o|B=r)p(B=r)}{p(F=o)} \\
&= \frac{3}{4} \times \frac{4}{10} \times \frac{20}{9} = \frac{2}{3} \tag{1.23}
\end{align*}
$$

- using the Bayes' theorem, we could make predictions about which box the fruit came
    - before we were told which fruit has been selected, the most complete information we had about box was $p(B)$
        - this is called *prior probability* because it is the probability available before we gain the data
    - after we observed the identity of the fruit, we could update our prior probability with the given data and obtained $p(B \vert F)$
        - this is called *posterior probability* because it in the probability obtained after we have observed F

- for this example, the probability of selecting the red box has significantly increased after we observed the selected fruit is an orange
    - this is becuase there were more oranges in the red box
    - this concept is very important later since we can update our prediction after we observe data
        - nobody wants to make poor guesses forever!
- two variables X and Y are *independent* if $p(X,Y) = p(X)p(Y)$
    - we can prove this from the product rule
    - if $p(X,Y) = p(X)p(Y)$, $p(Y \vert X) = p(Y)$
    - this means conditional probability of Y given X is independent of the value of X
